## MEAD 2.0 Release

### Goals for Version 2

- Support TF eager completely
  - TODO: figuring out a good/fast solution for seq2seq -- the TF1 `contrib` module way is much faster
- Better support for native Datasets
  - For TensorFlow, tf.dataset and still support multi-GPU
  - For PyTorch, use DataLoader
- Better a la carte support (providing layers that work outside of mead)
- Underlying layers API that is nearly identical between PyTorch and TF
  - Some cases pytorch requires explicit dims and TF no longer does which makes things more clunky than they need to be
- Get rid of python 2, make typing more explicit
- Improved documentation over previous offering
- Simplify by removing `dynet` support and non TF `keras` support
- Simplify services and provide a more straightforward `predict` like functionality with an array
- Updated API examples for accessing layers directly, including large-scale Transformer pre-training code with fine-tuning addons
- Comprehensive support for BERT and BPE-based Transformer models in TF and PyTorch
- Support for [mead-hub](https://github.com/mead-ml/hub), a centralized repository for sharing models, tasks, vectorizers and embeddings
  - [ELMo example](https://github.com/mead-ml/hub/blob/master/v1/addons/embed_elmo_tf.py) - by specifying this addon in your [mead config](https://github.com/dpressel/mead-baseline/blob/feature/v2/mead/config/sst2-elmo-eh.json)

*Note on TensorFlow Eager Mode*

By default, eager mode is turned off!  To turn it on, pass `--prefer-eager 1` into `mead-train`.  This only works in TF 2.x and recent versions of TF 1.x.  In cases where it is not supported, it should degrade gracefully


#### Open Items

- [ ] Fix handling of eager with SGD w/ Momentum and Adadelta
  - Sparse update ops dont work on GPU, In TF 2 eager mode this leads to errors, particularly with SGD w/ Momentum and Adadelta
    - https://github.com/tensorflow/tensorflow/issues/31291
    - This issues causes sub-optimal results in tagger configurations particularly, for which we have tuned the HPs specifically with momentum.  For this reason, we currently move the optimizers to the CPU, which causes a large slow down
      - *Update* The slowdown is mitigated using `tf.function`. PR coming soon
- [ ] Add PyTorch LM and seq2seq support for multiworker and large datasets from within `mead-train`
  - Right now, rely on API examples to do multiworker
- [ ] Clean-up TensorFlow multiworker estimator code and metric handling
 
